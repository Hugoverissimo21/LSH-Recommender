{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505c3a2c",
   "metadata": {},
   "source": [
    "Use CV Validation to get the best hyperparameters for the LSH model:\n",
    "\n",
    "- `bucketLength` $\\in [0.5, 2.0]$ based on some small research \n",
    "\n",
    "- `numHashTables` $\\in [1, 10]$ so the number of hash tables is not too large\n",
    "\n",
    "- `approxSimilarityJoin` threshold $\\in [0, 1.41]$ so the cosine angle can be at most $90 \\degree$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51538ba4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b63a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import collect_list, struct\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum as sql_sum, col\n",
    "from pyspark.sql.functions import coalesce, lit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b310d97c",
   "metadata": {},
   "source": [
    "# Treat the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa43846",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ItemItemCF\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = spark.read.csv(\"data/100k.csv\", header=True, inferSchema=True) \\\n",
    "            .select(\"userId\", \"movieId\", \"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d2fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 10 folds of data\n",
    "folds = data.randomSplit([0.1]*10, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7888433a",
   "metadata": {},
   "source": [
    "# Create functions automate the CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2e496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_item_cf_similarities(ratings, bucketLength_in, numHashTables_in, threshold_in):\n",
    "    # get the number of unique users\n",
    "    num_users = ratings.select(\"userId\").distinct().count()\n",
    "\n",
    "    # create the sparse vector for movie function\n",
    "    def to_sparse_vector(user_ratings, size):\n",
    "        # Sort by userId to get strictly increasing indices\n",
    "        sorted_pairs = sorted(user_ratings, key=lambda x: x.userId)\n",
    "        indices = [x.userId - 1 for x in sorted_pairs]\n",
    "        values = [x.rating for x in sorted_pairs]\n",
    "        return Vectors.sparse(size, indices, values)\n",
    "\n",
    "    # group by movieId and collect user ratings\n",
    "    item_user = ratings.groupBy(\"movieId\") \\\n",
    "        .agg(collect_list(struct(\"userId\", \"rating\")).alias(\"user_ratings\"))\n",
    "\n",
    "    # convert that to a sparse vector\n",
    "    item_vector_rdd = item_user.rdd.map(\n",
    "        lambda row: Row(\n",
    "            movieId=row[\"movieId\"],\n",
    "            features=to_sparse_vector(row[\"user_ratings\"], num_users)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # convert to DataFrame because of Normalizer (MLlib)\n",
    "    item_vectors = spark.createDataFrame(item_vector_rdd)\n",
    "\n",
    "    # normalizing with L2 (Euclidean) norm (p=2)\n",
    "    normalizer = Normalizer(inputCol=\"features\", outputCol=\"norm_features\", p=2.0)\n",
    "    normalized = normalizer.transform(item_vectors)\n",
    "\n",
    "    # create the LSH model\n",
    "    lsh = BucketedRandomProjectionLSH(\n",
    "        inputCol=\"norm_features\",\n",
    "        outputCol=\"hashes\",\n",
    "        bucketLength=bucketLength_in,\n",
    "        numHashTables=numHashTables_in\n",
    "    )\n",
    "\n",
    "    # fit the model\n",
    "    lsh_model = lsh.fit(normalized)\n",
    "\n",
    "    # get the approximate neighbors\n",
    "    neighbors = lsh_model.approxSimilarityJoin(\n",
    "        normalized,\n",
    "        normalized,\n",
    "        threshold=threshold_in,\n",
    "        distCol=\"distance\"\n",
    "    ).filter(col(\"datasetA.movieId\") < col(\"datasetB.movieId\"))  # avoid bottom triangle (reverse + self)\n",
    "\n",
    "    # convert the distance to cosine similarity\n",
    "    neighbors_cosine = neighbors.withColumn(\n",
    "        \"cosine_sim\",\n",
    "        1 - (col(\"distance\") ** 2) / 2\n",
    "    ).select(\n",
    "        col(\"datasetA.movieId\").alias(\"movie_i\"),\n",
    "        col(\"datasetB.movieId\").alias(\"movie_j\"),\n",
    "        \"cosine_sim\"\n",
    "    )\n",
    "\n",
    "    # add reverse pairs: (i,j) -> (i,j) and (j,i)\n",
    "    reverse = neighbors_cosine.selectExpr(\"movie_j as movie_i\", \"movie_i as movie_j\", \"cosine_sim\")\n",
    "    similarities = neighbors_cosine.union(reverse)\n",
    "\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90fa8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_item_cf_predictions(ratings, similarities, test):\n",
    "    # get the neighbors of the target movies\n",
    "    test_with_ratings = test.alias(\"t\") \\\n",
    "        .join(similarities.alias(\"s\"), col(\"t.movieId\") == col(\"s.movie_i\")) \\\n",
    "        .join(ratings.alias(\"r\"), (col(\"t.userId\") == col(\"r.userId\")) & (col(\"s.movie_j\") == col(\"r.movieId\"))) \\\n",
    "        .select(\n",
    "            col(\"t.userId\"),\n",
    "            col(\"t.movieId\").alias(\"target_movie\"),\n",
    "            col(\"s.movie_j\").alias(\"neighbor_movie\"),\n",
    "            col(\"s.cosine_sim\"),\n",
    "            col(\"r.rating\").alias(\"neighbor_rating\")\n",
    "        )\n",
    "\n",
    "    # get the predicted rating\n",
    "    predictions = test_with_ratings.groupBy(\"userId\", \"target_movie\").agg(\n",
    "        (sql_sum(col(\"cosine_sim\") * col(\"neighbor_rating\")) / sql_sum(col(\"cosine_sim\"))).alias(\"pred_rating\")\n",
    "    )\n",
    "\n",
    "    # join with the test set to get the actual rating\n",
    "    final = predictions.alias(\"p\").join(\n",
    "        test.alias(\"t\"),\n",
    "        (col(\"p.userId\") == col(\"t.userId\")) & (col(\"p.target_movie\") == col(\"t.movieId\")),\n",
    "        how=\"right\"  # keep all test rows even if no prediction (no neighbors)\n",
    "    ).select(\n",
    "        col(\"t.userId\"),\n",
    "        col(\"t.movieId\"),\n",
    "        coalesce(col(\"p.pred_rating\"), lit(3.0)).alias(\"pred_rating\"),\n",
    "        col(\"t.rating\").alias(\"actual_rating\")\n",
    "    )\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6ea656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_item_cf_results(final):\n",
    "    # RMSE\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"actual_rating\", predictionCol=\"pred_rating\")\n",
    "    rmse = evaluator.evaluate(final)\n",
    "\n",
    "    # MAE\n",
    "    mae_evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"actual_rating\", predictionCol=\"pred_rating\")\n",
    "    mae = mae_evaluator.evaluate(final)\n",
    "\n",
    "    return rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62e9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_item_cf_cv(train, test, hyperparameters):\n",
    "    bucketLength, numHashTables, threshold = hyperparameters\n",
    "\n",
    "    similarities = item_item_cf_similarities(train, bucketLength, numHashTables, threshold)\n",
    "    predictions = item_item_cf_predictions(train, similarities, test)\n",
    "    rmse, mae = item_item_cf_results(predictions)\n",
    "    \n",
    "    return rmse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f0e31",
   "metadata": {},
   "source": [
    "# Applying the CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fefde77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the tuning.json file\n",
    "if os.path.exists(\"tuning.json\"):\n",
    "    with open(\"tuning.json\", 'r') as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "else:\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d83dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the hyperparameters search space\n",
    "def sample_hyperparameters(n_samples):\n",
    "    bucketLength = np.random.uniform(0.5, 2.0, size=n_samples).round(1)\n",
    "    numHashTables = np.random.randint(1, 10, size=n_samples)\n",
    "    threshold = np.random.uniform(0.1, 1.41, size=n_samples).round(1)\n",
    "\n",
    "    return [(float(x), int(y), float(z)) for (x,y,z) in list(zip(bucketLength, numHashTables, threshold))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the hyperparameter tuning\n",
    "n_samples = 20\n",
    "hyperparameters = sample_hyperparameters(n_samples)\n",
    "\n",
    "# using partial CV for quicker tuning\n",
    "n_validations = 5\n",
    "\n",
    "# cycle through the hyperparameters\n",
    "for i, hyperparameter in enumerate(hyperparameters):\n",
    "    cvalidation = True\n",
    "\n",
    "    if True:\n",
    "        print(\"Bypassing hyperparameter tuning\")\n",
    "        break\n",
    "\n",
    "    bucketLength, numHashTables, threshold = hyperparameter\n",
    "    print(f\"Combination {i+1}/{n_samples}: bucketLength={bucketLength}, numHashTables={numHashTables}, threshold={threshold}\")\n",
    "\n",
    "    if str(hyperparameter) in results:\n",
    "        continue\n",
    "\n",
    "    avg_rmse, avg_mae, avg_time = 0, 0, 0\n",
    "    # cycle through the folds\n",
    "    for validation in range(n_validations):\n",
    "\n",
    "        try:\n",
    "            # get the training folds\n",
    "            training_folds = [folds[i] for i in range(len(folds)) if i not in [validation, 9]]\n",
    "            train = reduce(DataFrame.unionByName, training_folds)\n",
    "\n",
    "            # get the validation fold\n",
    "            val = folds[validation]\n",
    "\n",
    "            # run the item-item CF\n",
    "            start = time.time()\n",
    "            rmse, mae = item_item_cf_cv(train, val, hyperparameter)\n",
    "            end = time.time()\n",
    "\n",
    "            avg_rmse += rmse\n",
    "            avg_mae += mae\n",
    "            avg_time += round(end - start, 2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in combination {i+1}/{n_samples} for validation {validation}: {e}\")\n",
    "            cvalidation = False\n",
    "            break\n",
    "\n",
    "    # save the results\n",
    "    if cvalidation:\n",
    "        results[str(hyperparameter)] = {\n",
    "            \"bucketLength\": bucketLength,\n",
    "            \"numHashTables\": numHashTables,\n",
    "            \"threshold\": threshold,\n",
    "            \"n_validations\": n_validations,\n",
    "            \"rmse\": avg_rmse / n_validations,\n",
    "            \"mae\": avg_mae / n_validations,\n",
    "            \"time\": avg_time / n_validations\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tuning.json\", 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62608e3b",
   "metadata": {},
   "source": [
    "# Choosing the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ce1ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tuning.json\", 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.title(f\"Hyperparameter Tuning: Average Time vs RMSE ({len(results)} combinations)\")\n",
    "plt.xlabel(\"Average Time per Validation Fold (s)\")\n",
    "plt.ylabel(\"Partial CV RMSE\")\n",
    "\n",
    "for key, value in results.items():\n",
    "    plt.scatter(value[\"time\"], value[\"rmse\"], label=key)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad18881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDLE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
